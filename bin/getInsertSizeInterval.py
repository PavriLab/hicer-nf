# !/usr/bin/env python

import argparse as ap
import pandas as pd
import pyranges as pr
import pysam as ps
import numpy as np
import logging

# this is necessary since HICUP truncater does not cut away the RE cut site from reads and thus a truncated read
# overlaps with more than one REfragment reported by HICUP digester
def getFragmentsWithLargestOverlap(readsRanges, reFragmentRanges, keep):
    overlapIntervals = readsRanges.intersect(reFragmentRanges)
    overlapIntervals.Length = overlapIntervals.lengths()
    overlapIntervals = overlapIntervals.df

    # finding largest overlaps
    idx = overlapIntervals.groupby(['Name'], sort=False)['Length'].transform(max) == overlapIntervals['Length']
    # keeping last fragment if read1 or first if read2
    overlapIntervals = pr.PyRanges(df=overlapIntervals.loc[idx, :].drop_duplicates(subset='Name', keep=keep))


    return overlapIntervals.join(reFragmentRanges, how='left', suffix='_frag')\
                .df.loc[:, ['Chromosome', 'Start_frag', 'End_frag', 'Name']]


def computeInsertLengths(reads1, reads2, reFragmentRanges):
    reads1Ranges = pr.from_dict(reads1)
    reads2Ranges = pr.from_dict(reads2)

    reads1Fragments = getFragmentsWithLargestOverlap(reads1Ranges, reFragmentRanges, 'last')
    insertsReads1 = reads1Ranges.df.merge(reads1Fragments[['Start_frag', 'End_frag', 'Name']],
                                          on = 'Name', how = 'left')
    del reads1Ranges, reads1Fragments
    insertsReads1['Length'] = insertsReads1['End_frag'] - insertsReads1['Start']

    reads2Fragments =  getFragmentsWithLargestOverlap(reads2Ranges, reFragmentRanges, 'first')
    insertsReads2 = reads2Ranges.df.merge(reads2Fragments[['Start_frag', 'End_frag', 'Name']],
                                          on = 'Name', how = 'left')
    del reads2Ranges, reads2Fragments
    insertsReads2['Length'] = insertsReads2['End'] - insertsReads2['Start_frag']

    return insertsReads1.merge(insertsReads2, on = 'Name')[['Length_x', 'Length_y']].sum(axis = 1)


def writeSizeSummary(insertSizes, outputFileName):
    histogram, bins = np.histogram(insertSizes, bins=np.arange(0, 1500, 10))
    meanidx = np.argmax(histogram)

    width = 25
    symmetryValue = np.abs(meanidx - width) if meanidx - width < 0 else 0
    lowidx = meanidx - width if meanidx >= width else 0
    highidx = meanidx + width - symmetryValue
    lowBound, highBound = bins[lowidx], bins[highidx] + 10

    meanValue = insertSizes[insertSizes >= lowBound][insertSizes <= highBound].mean()
    standardDeviation = insertSizes[insertSizes >= lowBound][insertSizes <= highBound].std()

    minInsertSize = 10 - int(meanValue - standardDeviation * 3) % 10 + int(meanValue - standardDeviation * 3)
    maxInsertSize = 10 - int(meanValue + standardDeviation * 3) % 10 + int(meanValue + standardDeviation * 3)

    with open(outputFileName, 'w') as ofile:
        for tag, value in zip(['meanSize', 'stdSize', 'minInsertSize', 'maxInsertSize'],
                              [meanValue, standardDeviation, minInsertSize, maxInsertSize]):
            ofile.write('#' + tag + ':\t' + str(value) + '\n')

        ofile.write('\t'.join(['insertSize', 'numberOfOccurences']) + '\n')
        for insertSize, occurrence in insertSizes.value_counts().sort_index().items():
            ofile.write('{0}\t{1}\n'.format(insertSize, occurrence))


logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO)
parser = ap.ArgumentParser()
parser.add_argument('-i', '--inputSam', required = True,
                    help = 'samfile generated by hicup')
parser.add_argument('-d', '--digest', required = True,
                    help = 'digest file generated by hicup digest')
parser.add_argument('--batchSize', default = 1000000, type = int,
                    help = 'number of readpairs to read before computing insert size')
parser.add_argument('-o', '--outputFile', required = True,
                    help = 'name of the summary file to write')
args = parser.parse_args()

reFragments = pd.read_csv(args.digest,
                          sep = '\t',
                          skiprows = 2,
                          usecols = [0, 1, 2],
                          names = ['Chromosome', 'Start', 'End'])

reFragments.loc[:, 'Start'] = reFragments['Start'] - 1
reFragments = pr.PyRanges(df = reFragments)

with ps.AlignmentFile(args.inputSam, 'r') as inputSam:
    pairCount = 0
    reads1 = {k: [] for k in ['Chromosome', 'Start', 'End', 'Name']}
    reads2 = {k: [] for k in ['Chromosome', 'Start', 'End', 'Name']}
    insertSizes = pd.Series()
    while True:
        try:
            r1 = inputSam.__next__()
            r2 = inputSam.__next__()

        except StopIteration:
            tmpInsertSizes = computeInsertLengths(reads1, reads2, reFragments)
            reads1 = {k: [] for k in ['Chromosome', 'Start', 'End', 'Name']}
            reads2 = {k: [] for k in ['Chromosome', 'Start', 'End', 'Name']}

            insertSizes = pd.concat([insertSizes, tmpInsertSizes])
            break

        pairCount += 1

        for read, readDict in zip((r1, r2), (reads1, reads2)):
            readDict['Name'].append(pairCount)
            for k, v in zip(['Chromosome', 'Start', 'End'],
                            (read.reference_name, read.reference_start, read.reference_end)):
                readDict[k].append(v)

        if pairCount%args.batchSize == 0:
            tmpInsertSizes = computeInsertLengths(reads1, reads2, reFragments)
            reads1 = {k: [] for k in ['Chromosome', 'Start', 'End', 'Name']}
            reads2 = {k: [] for k in ['Chromosome', 'Start', 'End', 'Name']}

            insertSizes = pd.concat([insertSizes, tmpInsertSizes])
            logging.info('processed {0} read pairs'.format(pairCount))

writeSizeSummary(insertSizes, args.outputFile)