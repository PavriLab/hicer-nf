#!/usr/bin/env python

# MIT License
#
# Copyright (c) 2020 Tobias Neumann, Daniel Malzl
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

import argparse as ap
import pandas as pd
import pyranges as pr
import pysam as ps
import numpy as np
import logging

# this is necessary since HICUP truncater does not cut away the RE cut site from reads and thus a truncated read
# overlaps with more than one REfragment reported by HICUP digester
def getFragmentsWithLargestOverlap(readsRanges, reFragmentRanges, keep):
    overlapIntervals = readsRanges.intersect(reFragmentRanges)
    overlapIntervals.Length = overlapIntervals.lengths()
    overlapIntervals = overlapIntervals.df

    # finding largest overlaps
    idx = overlapIntervals.groupby(['Name'], sort=False)['Length'].transform(max) == overlapIntervals['Length']
    # keeping last fragment if read1 or first if read2
    overlapIntervals = pr.PyRanges(df=overlapIntervals.loc[idx, :].drop_duplicates(subset='Name', keep=keep))


    return overlapIntervals.join(reFragmentRanges, how='left', suffix='_frag')\
                .df.loc[:, ['Chromosome', 'Start_frag', 'End_frag', 'Name']]


def computeInsertLengths(reads1, reads2, reFragmentRanges):
    reads1Ranges = pr.from_dict(reads1)
    reads2Ranges = pr.from_dict(reads2)

    reads1Fragments = getFragmentsWithLargestOverlap(reads1Ranges, reFragmentRanges, 'last')
    insertsReads1 = reads1Ranges.df.merge(reads1Fragments[['Start_frag', 'End_frag', 'Name']],
                                          on = 'Name', how = 'left')
    del reads1Ranges, reads1Fragments
    insertsReads1['Length'] = insertsReads1['End_frag'] - insertsReads1['Start']

    reads2Fragments =  getFragmentsWithLargestOverlap(reads2Ranges, reFragmentRanges, 'first')
    insertsReads2 = reads2Ranges.df.merge(reads2Fragments[['Start_frag', 'End_frag', 'Name']],
                                          on = 'Name', how = 'left')
    del reads2Ranges, reads2Fragments
    insertsReads2['Length'] = insertsReads2['End'] - insertsReads2['Start_frag']

    return insertsReads1.merge(insertsReads2, on = 'Name')[['Length_x', 'Length_y']].sum(axis = 1)


def writeSizeSummary(insertSizes, outputFileName):
    histogram, bins = np.histogram(insertSizes, bins=np.arange(0, 1500, 10))
    maxidx = np.argmax(histogram)
    highBound = bins[2 * maxidx] + 10

    meanValue = insertSizes[insertSizes < highBound].mean()
    standardDeviation = insertSizes[insertSizes < highBound].std()

    minInsertSize = 10 - int(meanValue - standardDeviation * 3) % 10 + int(meanValue - standardDeviation * 3)
    maxInsertSize = 10 - int(meanValue + standardDeviation * 3) % 10 + int(meanValue + standardDeviation * 3)

    with open(outputFileName, 'w') as ofile:
        for tag, value in zip(['meanSize', 'stdSize', 'minInsertSize', 'maxInsertSize'],
                              [meanValue, standardDeviation, minInsertSize, maxInsertSize]):
            ofile.write('#' + tag + ':\t' + str(value) + '\n')

        ofile.write('\t'.join(['insertSize', 'numberOfOccurences']) + '\n')
        for insertSize, occurrence in insertSizes.value_counts().sort_index().items():
            ofile.write('{0}\t{1}\n'.format(insertSize, occurrence))


logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO)
parser = ap.ArgumentParser()
parser.add_argument('-i', '--inputSam', required = True,
                    help = 'samfile generated by hicup')
parser.add_argument('-d', '--digest', required = True,
                    help = 'digest file generated by hicup digest')
parser.add_argument('--batchSize', default = 1000000, type = int,
                    help = 'number of readpairs to read before computing insert size')
parser.add_argument('-o', '--outputFile', required = True,
                    help = 'name of the summary file to write')
args = parser.parse_args()

reFragments = pd.read_csv(args.digest,
                          sep = '\t',
                          skiprows = 2,
                          usecols = [0, 1, 2],
                          names = ['Chromosome', 'Start', 'End'])

reFragments.loc[:, 'Start'] = reFragments['Start'] - 1
reFragments = pr.PyRanges(df = reFragments)

with ps.AlignmentFile(args.inputSam, 'r') as inputSam:
    pairCount = 0
    reads1 = {k: [] for k in ['Chromosome', 'Start', 'End', 'Name']}
    reads2 = {k: [] for k in ['Chromosome', 'Start', 'End', 'Name']}
    insertSizes = pd.Series()
    while True:
        try:
            r1 = inputSam.__next__()
            r2 = inputSam.__next__()

        except StopIteration:
            tmpInsertSizes = computeInsertLengths(reads1, reads2, reFragments)
            reads1 = {k: [] for k in ['Chromosome', 'Start', 'End', 'Name']}
            reads2 = {k: [] for k in ['Chromosome', 'Start', 'End', 'Name']}

            insertSizes = pd.concat([insertSizes, tmpInsertSizes])
            break

        pairCount += 1

        for read, readDict in zip((r1, r2), (reads1, reads2)):
            readDict['Name'].append(pairCount)
            for k, v in zip(['Chromosome', 'Start', 'End'],
                            (read.reference_name, read.reference_start, read.reference_end)):
                readDict[k].append(v)

        if pairCount%args.batchSize == 0:
            tmpInsertSizes = computeInsertLengths(reads1, reads2, reFragments)
            reads1 = {k: [] for k in ['Chromosome', 'Start', 'End', 'Name']}
            reads2 = {k: [] for k in ['Chromosome', 'Start', 'End', 'Name']}

            insertSizes = pd.concat([insertSizes, tmpInsertSizes])
            logging.info('processed {0} read pairs'.format(pairCount))

writeSizeSummary(insertSizes, args.outputFile)
